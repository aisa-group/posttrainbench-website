<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PostTrainBench</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&family=PT+Mono&family=Space+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">


    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <div class="nav-content">
                <div class="nav-left">
                    <button class="hamburger" id="hamburger-btn" aria-label="Toggle menu">
                        <span class="hamburger-line"></span>
                        <span class="hamburger-line"></span>
                        <span class="hamburger-line"></span>
                    </button>
                    <div class="logo">PostTrain<span class="logo-accent">Bench</span></div>
                </div>
                <div class="nav-right">
                    <div class="nav-links" id="nav-links">
                        <a href="#leaderboard">Leaderboard</a>
                        <a href="#about">About</a>
                        <a href="#observations">Observations</a>
                        <a href="#team">Team</a>
                    </div>
                    <a href="https://github.com/aisa-group/PostTrainBench" target="_blank" class="nav-github" aria-label="GitHub">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                        </svg>
                    </a>
                    <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                        <svg class="theme-icon theme-icon-sun" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                        <svg class="theme-icon theme-icon-moon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                    </button>
                </div>
            </div>
        </div>
    </nav>

    <section class="hero">
        <div class="container">
            <h1 class="hero-title">
                <span class="title-main">PostTrain</span><span class="title-accent">Bench</span>
            </h1>
            <p class="hero-subtitle">Measuring how well AI agents can post-train language models</p>
            <div class="hero-description">
                <p>Can AI agents improve performance of base LLMs? We give each agent 4 small target LLMs, an H100 GPU, and 10 hours to post-train them.</p>
            </div>
            <div class="hero-buttons">
                <div class="btn-wrapper">
                    <button id="paper-btn" class="btn btn-primary">Read the Paper</button>
                    <span class="tooltip">Coming Soon</span>
                </div>
                <a href="https://github.com/aisa-group/PostTrainBench" target="_blank" class="btn btn-secondary">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                    </svg>
                    GitHub
                </a>
            </div>
        </div>
    </section>

    <!-- Leaderboard -->
    <section id="leaderboard" class="leaderboard">
        <div class="container">
            <h2 class="section-title">Leaderboard</h2>
            <div class="leaderboard-chart">
                <div class="leaderboard-chart-wrapper">
                    <canvas id="performanceChart"></canvas>
                </div>
                <p class="chart-footnote"><sup>1</sup> The average is taken across all post-trained LLMs (Qwen 3 1.7B, Qwen 3 4B, SmolLM3-3B, Gemma 3 4B IT) and benchmarks (AIME 2025, BFCL, GPQA Main, GSM8K, HumanEval). For each run, we ask a CLI agent to maximize the performance of a specific base LLM on a specific benchmark.</p>
                <p class="chart-footnote"><sup>2</sup> "Human Post-Trained" is not directly comparable to the rest since it usually exceeds the 10h + 1 GPU constraint.</p>
            </div>
            <div class="leaderboard-controls">
                <label class="model-select-label">Filter by model:</label>
                <div class="custom-dropdown">
                    <div class="dropdown-selected" id="model-select-display">
                        Average (all models)
                    </div>
                    <div class="dropdown-options" id="model-select-options">
                        <div class="dropdown-option active" data-value="average">Average (all models)</div>
                        <div class="dropdown-option" data-value="Qwen3-1.7B">Qwen3-1.7B</div>
                        <div class="dropdown-option" data-value="Qwen3-4B">Qwen3-4B</div>
                        <div class="dropdown-option" data-value="SmolLM3-3B">SmolLM3-3B</div>
                        <div class="dropdown-option" data-value="Gemma-3-4B">Gemma-3-4B</div>
                    </div>
                </div>
            </div>
            <div class="leaderboard-table">
                <table>
                    <thead>
                        <tr>
                            <th>Rank</th>
                            <th>Method</th>
                            <th>Average Score</th>
                            <th>AIME 2025</th>
                            <th>BFCL</th>
                            <th>GPQA Main</th>
                            <th>GSM8K</th>
                            <th>HumanEval</th>
                        </tr>
                    </thead>
                    <tbody id="leaderboard-data">
                        <!-- Data will be inserted here via js-->
                    </tbody>
                </table>
            </div>
            <p class="leaderboard-footnote">More agents coming soon...</p>

            <h3 class="section-subtitle">Detailed Breakdown by Benchmark</h3>
            <div class="leaderboard-chart">
                <div class="leaderboard-chart-wrapper">
                    <canvas id="detailedChart"></canvas>
                </div>
            </div>
        </div>
    </section>

    <!-- Time Spent Section -->
    <section id="time-spent" class="time-spent">
        <div class="container">
            <h2 class="section-title">Average Time Spent</h2>
            <p class="section-description">Time taken by each agent to complete post-training (out of 10 hours). <br> Different agents demonstrate varying levels of persistence - some give up well before the time limit expires.</p>
            <div class="leaderboard-chart">
                <div class="leaderboard-chart-wrapper">
                    <canvas id="timeSpentChart"></canvas>
                </div>
            </div>
        </div>
    </section>

    <section id="process-flow" class="process-flow">
        <div class="container">
            <h2 class="section-title">Pipeline</h2>
            <div class="pipeline-diagram">
                <img src="pipeline.svg" alt="PostTrainBench Pipeline Diagram" class="pipeline-svg">
            </div>
        </div>
    </section>

    <!-- Benchmarks -->
    <section id="benchmarks" class="tasks">
        <div class="container">
            <h2 class="section-title">Evaluation</h2>
            <p class="section-description">Post-trained models are evaluated across these benchmarks to measure improvement in reasoning, knowledge, and problem-solving capabilities. We use <a href="https://inspect.ai-safety-institute.org.uk/" target="_blank" rel="noopener">Inspect</a> for evaluation and respect each model's generation_config.json.</p>
            <div class="task-grid" id="task-grid">
                <!-- Benchmark cards will be inserted here via js-->
            </div>
        </div>
    </section>

    <section id="about" class="about">
        <div class="container">
            <h2 class="section-title">About</h2>
            <div class="about-content">
                <p>Post-Train Bench measures AI R&D automation by testing whether AI agents can successfully post-train other language models. Each agent receives 4 base models (Qwen 3 1.7B, Qwen 3 4B, SmolLM3-3B, and Gemma 3 4B), access to an H100 GPU, and a 10-hour time limit to improve model performance through post-training</p>
                <div class="setup-info">
                    <h3>Experimental Setup</h3>
                    <ul>
                        <li><strong>Models:</strong> Qwen 3 1.7B, Qwen 3 4B, SmolLM3-3B, Gemma 3 4B</li>
                        <li><strong>Hardware:</strong> Single H100 GPU per agent</li>
                        <li><strong>Time Limit:</strong> 10 hours per agent</li>
                        <li><strong>Evaluation:</strong> Average score across 5 benchmarks</li>
                        <li><strong>Agent scaffolds:</strong> Native CLI scaffolds (Claude Code for Claude models, Codex CLI for OpenAI, Gemini CLI for Gemini)</li>
                    </ul>
                </div>
                
                
                
            </div>
        </div>
    </section>

    <section id="observations" class="observations">
        <div class="container">
            <h2 class="section-title">Observations</h2>

            <h3 class="section-subtitle">Agent Behaviors</h3>
            <div class="observations-grid">
                <div class="observation-card">
                    <h4>Claude Opus 4.5</h4>
                    <span class="observation-tag">Most Structured</span>
                    <ul>
                        <li>Uses explicit todo lists to track progress</li>
                        <li>Web searches for best practices</li>
                        <li>Creates detailed implementation plans before coding</li>
                    </ul>
                </div>
                <div class="observation-card">
                    <h4>GPT-5.x Variants</h4>
                    <span class="observation-tag">Action-Oriented</span>
                    <ul>
                        <li>Immediately starts exploring files and datasets</li>
                        <li>"Plan update" checkpoints with bullet points</li>
                        <li>Less formal planning, more exploratory</li>
                    </ul>
                </div>
                <div class="observation-card">
                    <h4>Gemini 3 Pro</h4>
                    <span class="observation-tag">Quick to Execute</span>
                    <ul>
                        <li>Less planning overhead</li>
                        <li>Jumps directly into implementation</li>
                        <li>More failures due to less error anticipation</li>
                    </ul>
                </div>
                <div class="observation-card">
                    <h4>GPT-5.1 Codex Max</h4>
                    <span class="observation-tag">Best Performer</span>
                    <ul>
                        <li>Building proper dataset pipelines (55k+ samples)</li>
                        <li>Iterating on training scripts when errors occurred</li>
                        <li>Using appropriate hyperparams (gradient checkpointing, bf16)</li>
                    </ul>
                </div>
            </div>

            <h3 class="section-subtitle">Time & Trace Patterns</h3>
            <div class="observation-block">
                <p>Agents had 3-10 hour limits. Behaviors varied significantly:</p>
                <ul>
                    <li><strong>GPT-5.1-codex:</strong> Often ran extremely long traces (381k+ lines on BFCL)</li>
                    <li><strong>Claude:</strong> Regularly checked <code>timer.sh</code> for remaining time</li>
                    <li><strong>Gemini:</strong> Shorter traces, faster iteration but more failures</li>
                </ul>
            </div>

            <h3 class="section-subtitle">Reward Hacking (Near Misses)</h3>
            <div class="observation-block">
                <p>Claude found that <code>Qwen/Qwen3-1.7B</code> (the instruct-tuned version) works "perfectly" for function calling. However, it then explicitly acknowledged:</p>
                <blockquote>
                    "However, the user specifically said to use Qwen/Qwen3-1.7B-Base. Let me re-read the user's constraint... So I must use the BASE model."
                </blockquote>
                <p>All agents showed awareness of contamination rules:</p>
                <ul>
                    <li><strong>Claude:</strong> "Cannot use [benchmark] test data for training (data contamination)"</li>
                    <li><strong>GPT models:</strong> "avoid leaking evaluation data", "avoiding test contamination"</li>
                    <li>All agents sourced training data from alternative datasets (MBPP, glaive-function-calling, Hermes, etc.)</li>
                </ul>
            </div>

            <h3 class="section-subtitle">Key Takeaways</h3>
            <div class="takeaways-list">
                <div class="takeaway-item">
                    <span class="takeaway-number">1</span>
                    <p><strong>Dataset quality > training duration:</strong> GPT-5.1-codex-max's success came from careful dataset curation, not longer training</p>
                </div>
                <div class="takeaway-item">
                    <span class="takeaway-number">2</span>
                    <p><strong>Constraint awareness:</strong> Almost all agents showed understanding of rules and avoided contamination</p>
                </div>
                <div class="takeaway-item">
                    <span class="takeaway-number">3</span>
                    <p><strong>Self-correction:</strong> Claude showed self-correction that avoids reward hacking by model substitution</p>
                </div>
                <div class="takeaway-item">
                    <span class="takeaway-number">4</span>
                    <p><strong>Library issues:</strong> Many errors came from library version mismatches (trl, transformers)</p>
                </div>
                <div class="takeaway-item">
                    <span class="takeaway-number">5</span>
                    <p><strong>Format alignment matters:</strong> For function calling, matching exact output format was essential for high scores</p>
                </div>
                <div class="takeaway-item">
                    <span class="takeaway-number">6</span>
                    <p><strong>Longer traces ≠ better results:</strong> GPT-5.1-codex had longest traces but inconsistent results; GPT-5.1-codex-max had shorter traces but better outcomes</p>
                </div>
            </div>
        </div>
    </section>

    <section id="team" class="team">
        <div class="container">
            <h2 class="section-title">Team</h2>
            <div class="team-grid">
                <div class="team-member">
                    <a href="https://www.linkedin.com/in/ben-rank" class="team-member-name">Ben Rank</a>
                    <div class="team-member-affiliations">
                        ELLIS Institute Tübingen<br>
                        Max Planck Institute for Intelligent Systems<br>
                        Tübingen AI Center
                    </div>
                </div>
                <div class="team-member">
                    <a href="https://hrdkbhatnagar.github.io/" class="team-member-name">Hardik Bhatnagar</a>
                    <div class="team-member-affiliations">
                        University of Tübingen<br>
                        Tübingen AI Center
                    </div>
                </div>
                <div class="team-member">
                    <a href="https://scholar.google.de/citations?user=0z0fNxUAAAAJ&hl=en" class="team-member-name">Matthias Bethge</a>
                    <div class="team-member-affiliations">
                        University of Tübingen<br>
                        Tübingen AI Center
                    </div>
                </div>
                <div class="team-member">
                    <a href="https://www.andriushchenko.me/" class="team-member-name">Maksym Andriushchenko</a>
                    <div class="team-member-affiliations">
                        ELLIS Institute Tübingen<br>
                        Max Planck Institute for Intelligent Systems<br>
                        Tübingen AI Center
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="citation" class="citation">
        <div class="container">
            <h2 class="section-title">Citation</h2>
            <p class="section-description">If you found PostTrainBench useful, please cite us as:</p>
            <div class="citation-box">
                <pre class="citation-text">@misc{posttrainbench_2025,
  title={PostTrainBench: Measuring AI Ability to Perform LLM Post-Training},
  author={Rank, Ben and Bhatnagar, Hardik and Bethge, Matthias and Andriushchenko, Maksym},
  year={2025}
}</pre>
                <button class="copy-btn" id="copy-citation" aria-label="Copy citation">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                    Copy
                </button>
            </div>
        </div>
    </section>

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2"></script>
    <script src="data.js"></script>
    <script src="script.js"></script>
</body>
</html>
